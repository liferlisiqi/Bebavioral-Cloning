{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload:  ./pre-data.pickle\n",
      "X_train shape:  (24108, 80, 80, 3) y_train shape:  (24108,)\n"
     ]
    }
   ],
   "source": [
    "def reload_data(pickle_file):\n",
    "    print('reload: ', pickle_file)\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        pickle_data = pickle.load(f)\n",
    "        X_train = pickle_data['X_train']\n",
    "        y_train = pickle_data['y_train']\n",
    "        del pickle_data  # Free up memory\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train, y_train = reload_data('./pre-data.pickle')\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "print('X_train shape: ', X_train.shape, 'y_train shape: ',y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia = Sequential()\n",
    "nvidia.add(Lambda(lambda x: x/255. - 0.5, input_shape=(80, 80, 3)))\n",
    "nvidia.add(Cropping2D(cropping=((35, 13), (0, 0))))\n",
    "nvidia.add(Convolution2D(24, 3, 3, subsample=(2, 2), activation='relu'))\n",
    "nvidia.add(Convolution2D(36, 3, 3, subsample=(2, 2), activation='relu'))\n",
    "nvidia.add(Convolution2D(48, 3, 3, activation='relu'))\n",
    "nvidia.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "nvidia.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Flatten())\n",
    "nvidia.add(Dense(100))\n",
    "nvidia.add(Dense(50))\n",
    "nvidia.add(Dense(10))\n",
    "nvidia.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19286 samples, validate on 4822 samples\n",
      "Epoch 1/5\n",
      "19286/19286 [==============================] - 130s - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 2/5\n",
      "19286/19286 [==============================] - 10s - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 3/5\n",
      "19286/19286 [==============================] - 12s - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 4/5\n",
      "19286/19286 [==============================] - 12s - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 5/5\n",
      "19286/19286 [==============================] - 12s - loss: 0.0099 - val_loss: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5\n",
    "\n",
    "# Training\n",
    "nvidia.compile(loss='mse', optimizer=Adam(LEARNING_RATE))\n",
    "nvidia.fit(X_train, y_train, validation_split=0.2, \n",
    "           shuffle=True, nb_epoch=EPOCHS)\n",
    "nvidia.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Train the model with generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size):\n",
    "    ns = len(samples)\n",
    "    while 1:\n",
    "        samples = shuffle(samples)\n",
    "        for offset in range(0, ns, batch_size):\n",
    "            batch_samples = samples[offset:offset + batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "            for sample in batch_samples:\n",
    "                center_path = './sample_data/data/IMG/' + sample[0].split('/')[-1]\n",
    "                center_img = cv2.imread(center_path)\n",
    "                center_img = cv2.resize(center_img, None, fx=0.25, fy=0.5)\n",
    "                center_img = cv2.cvtColor(center_img, cv2.COLOR_BGR2RGB)\n",
    "                center_angle = float(line[3])\n",
    "                images.append(center_img)\n",
    "                angles.append(center_angle)\n",
    "                images.append(cv2.flip(center_img, 1))\n",
    "                angles.append(-center_angle)\n",
    "\n",
    "                left_path = './sample_data/data/IMG/' + sample[1].split('/')[-1]\n",
    "                left_img = cv2.imread(left_path)\n",
    "                left_img = cv2.resize(left_img, None, fx=0.25, fy=0.5)\n",
    "                left_img = cv2.cvtColor(left_img, cv2.COLOR_BGR2RGB)\n",
    "                left_angle = float(line[3]) + 0.20\n",
    "                images.append(left_img)\n",
    "                angles.append(left_angle)\n",
    "\n",
    "                right_path = './sample_data/data/IMG/' + sample[2].split('/')[-1]\n",
    "                right_img = cv2.imread(right_path)\n",
    "                right_img = cv2.resize(right_img, None, fx=0.25, fy=0.5)\n",
    "                right_img = cv2.cvtColor(right_img, cv2.COLOR_BGR2RGB)\n",
    "                right_angle = float(line[3]) - 0.25\n",
    "                images.append(right_img)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield shuffle(X_train, y_train)\n",
    "            \n",
    "train_generator = generator(train_lines, batch_size=128)\n",
    "valida_generator = generator(valida_lines, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 7\n",
    "\n",
    "# Training\n",
    "nvidia.compile(loss='mse', optimizer=Adam(LEARNING_RATE))\n",
    "nvidia.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(4*train_lines),\n",
    "                     validation_data=valida_generator, \n",
    "                     nb_val_samples=len(4*valida_lines), \n",
    "                     nb_epoch=EPOCHS)\n",
    "nvidia.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
